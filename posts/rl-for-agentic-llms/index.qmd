---
title: "My First Post"
date: 2026-01-02
categories: [Research, Notes]
description: "A short intro post and what I plan to write about."
format:
  html:
    fig-numbering: false
---

Reinforcement learning for LLMs is already a delicate process in single-step settings.
In long-horizon, multi-step agentic interactions, that delicacy often turns into outright training instability.
In this post, I reflect on the challenges that arise in this regime and share some of the approaches I am 
exploring to make training more stable.
The observations in this post draw on experiments that I set up together with Lukas Wutschitz, as well as 
many discussions along the way.

*Note:* This is the first post on this blog and is intended to be an informal discussion of ongoing work. 
If you spot errors, have alternative interpretations, or want to discuss related ideas, I would be very happy to hear from you.

## Experimental setting

All observations in this post are based on experiments using the [AppWorld benchmark](https://github.com/stonybrooknlp/appworld), 
a simulated environment designed to evaluate agentic language models on long-horizon, multi-step tasks involving 
real-world everyday apps and data from digital lives and activities of fictitious users. 

For training, I use an RL pipeline built on the awesome [rllm](https://github.com/rllm-org/rllm) library, which supports 
post-training of custom agents within custom environments via reinforcement learning.

## Potential sources of training instability

Training instability in reinforcement learning for agentic LLMs can arise from multiple, often compounding, factors. 
Below are several contributors that repeatedly surfaced in my experiments.

### 1. Gradient updates from non-useful trajectories

In long-horizon settings, a substantial fraction of collected trajectories may be effectively non-informative for learning. 
These include trajectories where the agent exhausts the maximum number of environment steps or generation tokens without making 
meaningful progress toward the task objective.

When such trajectories are used for policy updates, they can introduce significant noise into the gradient. Fortunately, this 
failure mode is relatively easy to detect in practice, and simple filtering strategies can already improve training stability.

### 2. Training–inference mismatch

A more subtle and widely studied source of instability arises from mismatches between the inference engines that generate the 
rollouts and the training frameworks that compute policy updates based on those rollouts.

There is already an excellent body of work analyzing this issue in depth, and I will not attempt to summarize it fully here. 
Instead, I point readers to several high-quality discussions that shaped my understanding of this failure mode:

- <https://fengyao.notion.site/off-policy-rl>
- <https://www.llmdata.com/blog/mismatch-praxis/>  
- <https://arxiv.org/pdf/2510.26788>  
- <https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda>  

In practice, these mismatches can lead to sudden collapses during RL training.

### 3. Retokenization drift

Another instability source that becomes particularly pronounced in long-horizon agentic settings is called retokenization drift. 
This phenomenon occurs when the tokenization of model-generated text during inference diverges from the tokenization used during training.

This phenomenon is explained clearly in a recent blog post on agentic inference systems:
<https://blog.vllm.ai/2025/10/22/agent-lightning.html>

## Directions for improving stability

To isolate instability sources without introducing unnecessary complexity, I start from a strong instruction-tuned base model and focus on 
a strictly on-policy training regime. Concretely, I initialize the agent from Qwen3-4B-Instruct-2507 and apply an on-policy variant of the 
GRPO algorithm. All experiments use the full training and validation splits of the AppWorld benchmark.

### Filtering non-useful trajectories
The simplest intervention I explored is filtering out non-useful trajectories before computing policy updates.
Specifically, I discard trajectories where the agent either (i) exhausts the maximum number of environment steps or 
(ii) generates the maximum number of tokens without completing the task. This simple strategy alone gives the following results:

```{=html}
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p2.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p3.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
```

First of all, we do not observe any sudden training collapses (at least within the first 500 RL iterations). However, the gradient norms remain 
quite high throughout training, indicating that the training process is still rather unstable. The training mean reward gets around 0.5 and 
validation task completion rate is around 30% after 500 RL iterations, which is a decent but not satisfactory performance.

### + retokenization-drift mitigated training
Fortunately, with rllm v0.2.1, the retokenization drift issue has been addressed via <https://github.com/rllm-org/rllm/pull/272>.
If the steps are not cumulative, first differing position is identified, and the trajectory is truncated there. The user can also
choose whether to keep or discard the truncated trajectory for training via the `filter_token_mismatch` flag. Let's start with 
`filter_token_mismatch=True`, i.e., the default option of discarding the truncated trajectories. With both trajectory filtering 
and retokenization-drift mitigated training enabled, training becomes significantly more stable:

```{=html}
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p2.png" class="figure-img img-fluid" alt="Actor entropy">
    <figcaption class="figure-caption">Actor entropy</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p3.png" class="figure-img img-fluid" alt="Tokenization mismatch mean">
    <figcaption class="figure-caption">Tokenization mismatch mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p4.png" class="figure-img img-fluid" alt="Rollout probs diff mean">
    <figcaption class="figure-caption">Rollout probs diff mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p5.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p6.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
```

We observe that the gradient norms are lower than 1.0 throughout training except very few spikes. We also obtain much larger
training mean reward (~0.8) and validation task completion rate (~60%) after 500 RL iterations. We get a behavior change 
after 300 steps where entropy, tokenization mismatch, and rollout probs diff start to increase but I don't have a good explanation 
for that yet.

One important question is that why there is still a non-zero tokenization mismatch even after getting token ids from vllm. 
The problem occurs due to the multi-step loop structure as the response token ids are detokenized to be sent to the environment 
and after getting the next observation, they are again tokenized for the next step generation. The mismatch occurs during this
detokenization–tokenization cycle. There is some work to do within `rllm` library to keep the whole multi-step loop with token ids.

Let's also see what happens if we keep the truncated trajectories instead of discarding them by setting `filter_token_mismatch=False`:
```{=html}
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p2.png" class="figure-img img-fluid" alt="Actor entropy">
    <figcaption class="figure-caption">Actor entropy</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p3.png" class="figure-img img-fluid" alt="Tokenization mismatch mean">
    <figcaption class="figure-caption">Tokenization mismatch mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p4.png" class="figure-img img-fluid" alt="Rollout probs diff mean">
    <figcaption class="figure-caption">Rollout probs diff mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p5.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p6.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
```

Interestingly, we observe pretty different behavior in entropy, tokenization mismatch, and rollout probs diff compared to the previous case. 
However, the overall training stability remains quite good, and we obtain slightly higher validation task completion rate (~61-64%) after 
500 RL iterations. This suggests that even the truncated trajectories contain some useful learning signals, although further analysis is needed 
to fully understand their impact.

### ++ Rollout correction methods for training–inference mismatch

So far in the experiments presented, I have not applied any rollout correction methods to address training–inference mismatch. Checking rollout 
probs diff mean plots, we can see that there is a non-negligible mismatch between the inference and training distributions although the training 
stability is already significantly improved with retokenization-drift mitigated training.

For now, I will leave the investigation of training–inference mismatch and associated rollout correction methods as future work. 
This should be an extensive study given a growing body of work on the topic: <https://verl.readthedocs.io/en/latest/algo/rollout_corr_math.html>.