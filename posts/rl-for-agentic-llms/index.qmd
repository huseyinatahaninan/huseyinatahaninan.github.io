---
title: "Training Instability in Long-Horizon Agentic RL (Notes from Practice)"
date: 2026-01-05
description: "Observations and lessons from reinforcement learning experiments with long-horizon agentic LLMs."
---

Reinforcement learning for LLMs is already a delicate process in single-step settings.
In long-horizon, multi-step agentic interactions, that delicacy often turns into outright training instability.
In this post, I reflect on the challenges that arise in this regime and share some of the approaches I am 
exploring to make training more stable.
The observations in this post draw on experiments that I set up together with **Lukas Wutschitz**, as well as 
many discussions along the way.

*Note:* This is the first post on this blog and is intended to be an informal discussion of ongoing work. 
If you spot errors, have alternative interpretations, or want to discuss related ideas, I would be very happy to hear from you.

## Experimental setting

All observations in this post are based on experiments conducted using the [AppWorld benchmark](https://github.com/stonybrooknlp/appworld), 
a simulated environment designed to evaluate agentic language models on long-horizon, multi-step tasks involving 
real-world everyday apps and data from digital lives and activities of fictitious users. 

For training, I use an RL pipeline built on the excellent [rllm](https://github.com/rllm-org/rllm) library, which supports 
post-training of custom agents within custom environments via reinforcement learning.

## Potential sources of training instability

Training instability in reinforcement learning for agentic LLMs can arise from multiple, often compounding, factors. 
Below are several contributors that repeatedly surfaced in my experiments.

### 1. Gradient updates from non-useful trajectories

In long-horizon settings, a substantial fraction of collected trajectories may be effectively non-informative for learning. 
These include trajectories where the agent exhausts the maximum number of environment steps or generation tokens without making 
meaningful progress toward the task objective.

When such trajectories are used for policy updates, they can introduce significant noise into the gradient. Fortunately, this 
failure mode is relatively easy to detect in practice, and simple filtering strategies can already improve training stability.

### 2. Training–inference mismatch

A more subtle and widely studied source of instability arises from mismatches between the inference engines that generate the 
rollouts and the training frameworks that compute policy updates based on those rollouts.

There is already an excellent body of work analyzing this issue in depth, and I will not attempt to summarize it fully here. 
Instead, I point readers to several high-quality discussions that shaped my understanding of this failure mode:

- <https://fengyao.notion.site/off-policy-rl>
- <https://www.llmdata.com/blog/mismatch-praxis/>  
- <https://arxiv.org/pdf/2510.26788>  
- <https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda>  

In practice, these mismatches can lead to sudden collapses during RL training.

### 3. Retokenization drift

Another instability source that becomes particularly pronounced in long-horizon agentic settings is called retokenization drift. 
This phenomenon occurs when the tokenization of model-generated text during inference diverges from the tokenization used during training.

This issue is explained clearly in a recent blog post on agentic inference systems:
<https://blog.vllm.ai/2025/10/22/agent-lightning.html>

## Directions for improving stability

To isolate instability sources without introducing unnecessary complexity, I start from a strong instruction-tuned base model and focus on 
a strictly on-policy training regime. Concretely, I initialize the agent from Qwen3-4B-Instruct-2507 and apply an on-policy variant of the 
GRPO algorithm. All experiments use the full training and validation splits of the AppWorld benchmark.

### Filtering non-useful trajectories
The simplest intervention I explored is filtering out non-useful trajectories before computing policy updates.
Specifically, I discard trajectories where the agent either (i) exhausts the maximum number of environment steps or 
(ii) generates the maximum number of tokens without completing the task. 
This simple strategy alone yields the following results:

```{=html}
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p2.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p3.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
```

First, we do not observe any sudden training collapses (at least within the first 500 RL iterations). However, the gradient norms remain 
quite high throughout training, indicating that the training process is still rather unstable. The training mean reward reaches around 0.5 and 
validation task completion rate is around 30% after 500 RL iterations, which is a decent but not satisfactory performance.

### + retokenization-drift mitigated training
With rllm v0.2.1, the retokenization drift issue has been addressed via <https://github.com/rllm-org/rllm/pull/272>.
If the token sequences are not cumulative, the first differing position is identified, and the trajectory is truncated at that point. 
Users can also choose whether to keep or discard the truncated trajectory for training via the `filter_token_mismatch` flag. I begin with 
`filter_token_mismatch=True`, i.e., the default option of discarding the truncated trajectories. With both trajectory filtering 
and retokenization-drift mitigated training enabled, training becomes significantly more stable:

```{=html}
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p2.png" class="figure-img img-fluid" alt="Actor entropy">
    <figcaption class="figure-caption">Actor entropy</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p3.png" class="figure-img img-fluid" alt="Tokenization mismatch mean">
    <figcaption class="figure-caption">Tokenization mismatch mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p4.png" class="figure-img img-fluid" alt="Rollout probs diff mean">
    <figcaption class="figure-caption">Rollout probs diff mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p5.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p6.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
```

We observe that gradient norms remain below 1.0 throughout training, aside from a few isolated spikes. 
Training mean reward increases to around 0.8 and validation task completion rate reaches approximately 60% after 500 RL iterations. 
A behavior change occurs after roughly 300 iterations, where entropy, tokenization mismatch, and rollout probability differences begin to increase. 
At the moment, I do not have a clear explanation for this behavior.

One remaining question is why a non-zero tokenization mismatch persists even when token IDs are obtained directly from vLLM. 
This appears to arise from the multi-step interaction loop: response token IDs are detokenized to interact with the environment and then 
re-tokenized when generating the next step. 
The mismatch occurs during this detokenization–re-tokenization cycle. 
Addressing this fully likely requires keeping the entire multi-step loop in token space within the `rllm` library.

Finally, I explore what happens when truncated trajectories are kept rather than discarded by setting `filter_token_mismatch=False`:
```{=html}
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p2.png" class="figure-img img-fluid" alt="Actor entropy">
    <figcaption class="figure-caption">Actor entropy</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p3.png" class="figure-img img-fluid" alt="Tokenization mismatch mean">
    <figcaption class="figure-caption">Tokenization mismatch mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p4.png" class="figure-img img-fluid" alt="Rollout probs diff mean">
    <figcaption class="figure-caption">Rollout probs diff mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p5.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p6.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
```

Interestingly, we observe notably different behavior in entropy, tokenization mismatch, and rollout probability differences compared 
to the previous setting. 
However, overall training stability remains strong, and the validation task completion rate improves slightly to approximately 61–64% 
after 500 RL iterations. 
This suggests that even truncated trajectories may still contain useful learning signals, although further analysis is needed to fully 
understand their impact.

### ++ Rollout correction methods for training–inference mismatch

So far, none of the experiments presented apply rollout correction methods to explicitly address training–inference mismatch. 
Inspecting the rollout probability difference plots, we observe a non-negligible mismatch between inference-time and training-time distributions, 
even though overall training stability has improved substantially.

A careful study of training–inference mismatch and rollout correction methods is left for future work, given the growing body of recent methods 
in this area: <https://verl.readthedocs.io/en/latest/algo/rollout_corr_math.html>.