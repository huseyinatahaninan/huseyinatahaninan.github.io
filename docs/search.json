[
  {
    "objectID": "posts/rl-for-agentic-llms/index.html",
    "href": "posts/rl-for-agentic-llms/index.html",
    "title": "Training Instability in Long-Horizon Agentic RL (Notes from Practice)",
    "section": "",
    "text": "Reinforcement learning for LLMs is already a delicate process in single-step settings. In long-horizon, multi-step agentic interactions, that delicacy often turns into outright training instability. In this post, I reflect on the challenges that arise in this regime and share some of the approaches I am exploring to make training more stable. The observations in this post draw on experiments that I set up together with Lukas Wutschitz, as well as many discussions along the way.\nNote: This is the first post on this blog and is intended to be an informal discussion of ongoing work. If you spot errors, have alternative interpretations, or want to discuss related ideas, I would be very happy to hear from you."
  },
  {
    "objectID": "posts/rl-for-agentic-llms/index.html#experimental-setting",
    "href": "posts/rl-for-agentic-llms/index.html#experimental-setting",
    "title": "Training Instability in Long-Horizon Agentic RL (Notes from Practice)",
    "section": "Experimental setting",
    "text": "Experimental setting\nAll observations in this post are based on experiments conducted using the AppWorld benchmark, a simulated environment designed to evaluate agentic language models on long-horizon, multi-step tasks involving real-world everyday apps and data from digital lives and activities of fictitious users.\nFor training, I use an RL pipeline built on the excellent rllm library, which supports post-training of custom agents within custom environments via reinforcement learning."
  },
  {
    "objectID": "posts/rl-for-agentic-llms/index.html#potential-sources-of-training-instability",
    "href": "posts/rl-for-agentic-llms/index.html#potential-sources-of-training-instability",
    "title": "Training Instability in Long-Horizon Agentic RL (Notes from Practice)",
    "section": "Potential sources of training instability",
    "text": "Potential sources of training instability\nTraining instability in reinforcement learning for agentic LLMs can arise from multiple, often compounding, factors. Below are several contributors that repeatedly surfaced in my experiments.\n\n1. Gradient updates from non-useful trajectories\nIn long-horizon settings, a substantial fraction of collected trajectories may be effectively non-informative for learning. These include trajectories where the agent exhausts the maximum number of environment steps or generation tokens without making meaningful progress toward the task objective.\nWhen such trajectories are used for policy updates, they can introduce significant noise into the gradient. Fortunately, this failure mode is relatively easy to detect in practice, and simple filtering strategies can already improve training stability.\n\n\n2. Training–inference mismatch\nA more subtle and widely studied source of instability arises from mismatches between the inference engines that generate the rollouts and the training frameworks that compute policy updates based on those rollouts.\nThere is already an excellent body of work analyzing this issue in depth, and I will not attempt to summarize it fully here. Instead, I point readers to several high-quality discussions that shaped my understanding of this failure mode:\n\nhttps://fengyao.notion.site/off-policy-rl\nhttps://www.llmdata.com/blog/mismatch-praxis/\n\nhttps://arxiv.org/pdf/2510.26788\n\nhttps://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda\n\nIn practice, these mismatches can lead to sudden collapses during RL training.\n\n\n3. Retokenization drift\nAnother instability source that becomes particularly pronounced in long-horizon agentic settings is called retokenization drift. This phenomenon occurs when the tokenization of model-generated text during inference diverges from the tokenization used during training.\nThis issue is explained clearly in a recent blog post on agentic inference systems: https://blog.vllm.ai/2025/10/22/agent-lightning.html"
  },
  {
    "objectID": "posts/rl-for-agentic-llms/index.html#directions-for-improving-stability",
    "href": "posts/rl-for-agentic-llms/index.html#directions-for-improving-stability",
    "title": "Training Instability in Long-Horizon Agentic RL (Notes from Practice)",
    "section": "Directions for improving stability",
    "text": "Directions for improving stability\nTo isolate instability sources without introducing unnecessary complexity, I start from a strong instruction-tuned base model and focus on a strictly on-policy training regime. Concretely, I initialize the agent from Qwen3-4B-Instruct-2507 and apply an on-policy variant of the GRPO algorithm. All experiments use the full training and validation splits of the AppWorld benchmark.\n\nFiltering non-useful trajectories\nThe simplest intervention I explored is filtering out non-useful trajectories before computing policy updates. Specifically, I discard trajectories where the agent either (i) exhausts the maximum number of environment steps or (ii) generates the maximum number of tokens without completing the task. This simple strategy alone yields the following results:\n\n  \n    \n    Actor gradient norm\n  \n  \n    \n    Training mean reward\n  \n  \n    \n    Validation task completion score\n  \n\nFirst, we do not observe any sudden training collapses (at least within the first 500 RL iterations). However, the gradient norms remain quite high throughout training, indicating that the training process is still rather unstable. The training mean reward reaches around 0.5 and validation task completion rate is around 30% after 500 RL iterations, which is a decent but not satisfactory performance.\n\n\n+ retokenization-drift mitigated training\nWith rllm v0.2.1, the retokenization drift issue has been addressed via https://github.com/rllm-org/rllm/pull/272. If the token sequences are not cumulative, the first differing position is identified, and the trajectory is truncated at that point. Users can also choose whether to keep or discard the truncated trajectory for training via the filter_token_mismatch flag. I begin with filter_token_mismatch=True, i.e., the default option of discarding the truncated trajectories. With both trajectory filtering and retokenization-drift mitigated training enabled, training becomes significantly more stable:\n\n  \n    \n    Actor gradient norm\n  \n  \n    \n    Actor entropy\n  \n  \n    \n    Tokenization mismatch mean\n  \n  \n    \n    Rollout probs diff mean\n  \n  \n    \n    Training mean reward\n  \n  \n    \n    Validation task completion score\n  \n\nWe observe that gradient norms remain below 1.0 throughout training, aside from a few isolated spikes. Training mean reward increases to around 0.8 and validation task completion rate reaches approximately 60% after 500 RL iterations. A behavior change occurs after roughly 300 iterations, where entropy, tokenization mismatch, and rollout probability differences begin to increase. At the moment, I do not have a clear explanation for this behavior.\nOne remaining question is why a non-zero tokenization mismatch persists even when token IDs are obtained directly from vLLM. This appears to arise from the multi-step interaction loop: response token IDs are detokenized to interact with the environment and then re-tokenized when generating the next step. The mismatch occurs during this detokenization–re-tokenization cycle. Addressing this fully likely requires keeping the entire multi-step loop in token space within the rllm library.\nFinally, I explore what happens when truncated trajectories are kept rather than discarded by setting filter_token_mismatch=False:\n\n  \n    \n    Actor gradient norm\n  \n  \n    \n    Actor entropy\n  \n  \n    \n    Tokenization mismatch mean\n  \n  \n    \n    Rollout probs diff mean\n  \n  \n    \n    Training mean reward\n  \n  \n    \n    Validation task completion score\n  \n\nInterestingly, we observe notably different behavior in entropy, tokenization mismatch, and rollout probability differences compared to the previous setting. However, overall training stability remains strong, and the validation task completion rate improves slightly to approximately 61–64% after 500 RL iterations. This suggests that even truncated trajectories may still contain useful learning signals, although further analysis is needed to fully understand their impact.\n\n\n++ Rollout correction methods for training–inference mismatch\nSo far, none of the experiments presented apply rollout correction methods to explicitly address training–inference mismatch. Inspecting the rollout probability difference plots, we observe a non-negligible mismatch between inference-time and training-time distributions, even though overall training stability has improved substantially.\nA careful study of training–inference mismatch and rollout correction methods is left for future work, given the growing body of recent methods in this area: https://verl.readthedocs.io/en/latest/algo/rollout_corr_math.html."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Training Instability in Long-Horizon Agentic RL (Notes from Practice)\n\n\nObservations and lessons from reinforcement learning experiments with long-horizon agentic LLMs.\n\n\n\n\n\nJan 5, 2026\n\n7 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Huseyin Atahan Inan",
    "section": "",
    "text": "I am a Principal Researcher at M365 Research, where I work on designing reinforcement learning (RL) methods for agentic large language models (LLMs). My current research focuses on strengthening agents’ task-completion and decision-making capabilities, while ensuring the appropriate and responsible use of user data.\nPreviously, I contributed to privacy-preserving machine learning (PPML) efforts, building and optimizing LLMs for a broad range of NLP tasks with differential privacy (DP) at the core. Highlights include:\n\nModernizing the Editor Text Predictions model with a transformer architecture fine-tuned under differential privacy, improving predictive accuracy while reducing operational costs across all Outlook endpoints.\nDeveloping privacy-preserving synthetic data pipelines by fine-tuning LLMs with differential privacy to generate high-quality data for classification and clustering; this work was featured in a Microsoft Research blog post.\n\n\n\n\nBefore joining Microsoft, I received my Ph.D. from Stanford University in 2020, where I was advised by Prof. Ayfer Özgür. Prior to that, I earned my M.Sc. from Koç University in 2014 under the supervision of Prof. Alper Erdoğan, and my B.Sc. from Boğaziçi University in 2012."
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Huseyin Atahan Inan",
    "section": "",
    "text": "I am a Principal Researcher at M365 Research, where I work on designing reinforcement learning (RL) methods for agentic large language models (LLMs). My current research focuses on strengthening agents’ task-completion and decision-making capabilities, while ensuring the appropriate and responsible use of user data.\nPreviously, I contributed to privacy-preserving machine learning (PPML) efforts, building and optimizing LLMs for a broad range of NLP tasks with differential privacy (DP) at the core. Highlights include:\n\nModernizing the Editor Text Predictions model with a transformer architecture fine-tuned under differential privacy, improving predictive accuracy while reducing operational costs across all Outlook endpoints.\nDeveloping privacy-preserving synthetic data pipelines by fine-tuning LLMs with differential privacy to generate high-quality data for classification and clustering; this work was featured in a Microsoft Research blog post."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Huseyin Atahan Inan",
    "section": "",
    "text": "Before joining Microsoft, I received my Ph.D. from Stanford University in 2020, where I was advised by Prof. Ayfer Özgür. Prior to that, I earned my M.Sc. from Koç University in 2014 under the supervision of Prof. Alper Erdoğan, and my B.Sc. from Boğaziçi University in 2012."
  }
]