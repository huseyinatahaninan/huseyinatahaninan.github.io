<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2026-01-05">
<meta name="description" content="Observations and lessons from reinforcement learning experiments with long-horizon agentic LLMs.">

<title>Training Instability in Long-Horizon Agentic RL (Notes from Practice)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-45a48b56c8ad2523a9a31c69be39928e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#experimental-setting" id="toc-experimental-setting" class="nav-link active" data-scroll-target="#experimental-setting">Experimental setting</a></li>
  <li><a href="#potential-sources-of-training-instability" id="toc-potential-sources-of-training-instability" class="nav-link" data-scroll-target="#potential-sources-of-training-instability">Potential sources of training instability</a></li>
  <li><a href="#directions-for-improving-stability" id="toc-directions-for-improving-stability" class="nav-link" data-scroll-target="#directions-for-improving-stability">Directions for improving stability</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Training Instability in Long-Horizon Agentic RL (Notes from Practice)</h1>
</div>

<div>
  <div class="description">
    Observations and lessons from reinforcement learning experiments with long-horizon agentic LLMs.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 5, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Reinforcement learning for LLMs is already a delicate process in single-step settings. In long-horizon, multi-step agentic interactions, that delicacy often turns into outright training instability. In this post, I reflect on the challenges that arise in this regime and share some of the approaches I am exploring to make training more stable. The observations in this post draw on experiments that I set up together with <strong>Lukas Wutschitz</strong>, as well as many discussions along the way.</p>
<p><em>Note:</em> This is the first post on this blog and is intended to be an informal discussion of ongoing work. If you spot errors, have alternative interpretations, or want to discuss related ideas, I would be very happy to hear from you.</p>
<section id="experimental-setting" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setting">Experimental setting</h2>
<p>All observations in this post are based on experiments conducted using the <a href="https://github.com/stonybrooknlp/appworld">AppWorld benchmark</a>, a simulated environment designed to evaluate agentic language models on long-horizon, multi-step tasks involving real-world everyday apps and data from digital lives and activities of fictitious users.</p>
<p>For training, I use an RL pipeline built on the excellent <a href="https://github.com/rllm-org/rllm">rllm</a> library, which supports post-training of custom agents within custom environments via reinforcement learning.</p>
</section>
<section id="potential-sources-of-training-instability" class="level2">
<h2 class="anchored" data-anchor-id="potential-sources-of-training-instability">Potential sources of training instability</h2>
<p>Training instability in reinforcement learning for agentic LLMs can arise from multiple, often compounding, factors. Below are several contributors that repeatedly surfaced in my experiments.</p>
<section id="gradient-updates-from-non-useful-trajectories" class="level3">
<h3 class="anchored" data-anchor-id="gradient-updates-from-non-useful-trajectories">1. Gradient updates from non-useful trajectories</h3>
<p>In long-horizon settings, a substantial fraction of collected trajectories may be effectively non-informative for learning. These include trajectories where the agent exhausts the maximum number of environment steps or generation tokens without making meaningful progress toward the task objective.</p>
<p>When such trajectories are used for policy updates, they can introduce significant noise into the gradient. Fortunately, this failure mode is relatively easy to detect in practice, and simple filtering strategies can already improve training stability.</p>
</section>
<section id="traininginference-mismatch" class="level3">
<h3 class="anchored" data-anchor-id="traininginference-mismatch">2. Training–inference mismatch</h3>
<p>A more subtle and widely studied source of instability arises from mismatches between the inference engines that generate the rollouts and the training frameworks that compute policy updates based on those rollouts.</p>
<p>There is already an excellent body of work analyzing this issue in depth, and I will not attempt to summarize it fully here. Instead, I point readers to several high-quality discussions that shaped my understanding of this failure mode:</p>
<ul>
<li><a href="https://fengyao.notion.site/off-policy-rl" class="uri">https://fengyao.notion.site/off-policy-rl</a></li>
<li><a href="https://www.llmdata.com/blog/mismatch-praxis/" class="uri">https://www.llmdata.com/blog/mismatch-praxis/</a><br>
</li>
<li><a href="https://arxiv.org/pdf/2510.26788" class="uri">https://arxiv.org/pdf/2510.26788</a><br>
</li>
<li><a href="https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda" class="uri">https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda</a></li>
</ul>
<p>In practice, these mismatches can lead to sudden collapses during RL training.</p>
</section>
<section id="retokenization-drift" class="level3">
<h3 class="anchored" data-anchor-id="retokenization-drift">3. Retokenization drift</h3>
<p>Another instability source that becomes particularly pronounced in long-horizon agentic settings is called retokenization drift. This phenomenon occurs when the tokenization of model-generated text during inference diverges from the tokenization used during training.</p>
<p>This issue is explained clearly in a recent blog post on agentic inference systems: <a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html" class="uri">https://blog.vllm.ai/2025/10/22/agent-lightning.html</a></p>
</section>
</section>
<section id="directions-for-improving-stability" class="level2">
<h2 class="anchored" data-anchor-id="directions-for-improving-stability">Directions for improving stability</h2>
<p>To isolate instability sources without introducing unnecessary complexity, I start from a strong instruction-tuned base model and focus on a strictly on-policy training regime. Concretely, I initialize the agent from Qwen3-4B-Instruct-2507 and apply an on-policy variant of the GRPO algorithm. All experiments use the full training and validation splits of the AppWorld benchmark.</p>
<section id="filtering-non-useful-trajectories" class="level3">
<h3 class="anchored" data-anchor-id="filtering-non-useful-trajectories">Filtering non-useful trajectories</h3>
<p>The simplest intervention I explored is filtering out non-useful trajectories before computing policy updates. Specifically, I discard trajectories where the agent either (i) exhausts the maximum number of environment steps or (ii) generates the maximum number of tokens without completing the task. This simple strategy alone yields the following results:</p>
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p2.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/baseline/p3.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
<p>First, we do not observe any sudden training collapses (at least within the first 500 RL iterations). However, the gradient norms remain quite high throughout training, indicating that the training process is still rather unstable. The training mean reward reaches around 0.5 and validation task completion rate is around 30% after 500 RL iterations, which is a decent but not satisfactory performance.</p>
</section>
<section id="retokenization-drift-mitigated-training" class="level3">
<h3 class="anchored" data-anchor-id="retokenization-drift-mitigated-training">+ retokenization-drift mitigated training</h3>
<p>With rllm v0.2.1, the retokenization drift issue has been addressed via <a href="https://github.com/rllm-org/rllm/pull/272" class="uri">https://github.com/rllm-org/rllm/pull/272</a>. If the token sequences are not cumulative, the first differing position is identified, and the trajectory is truncated at that point. Users can also choose whether to keep or discard the truncated trajectory for training via the <code>filter_token_mismatch</code> flag. I begin with <code>filter_token_mismatch=True</code>, i.e., the default option of discarding the truncated trajectories. With both trajectory filtering and retokenization-drift mitigated training enabled, training becomes significantly more stable:</p>
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p2.png" class="figure-img img-fluid" alt="Actor entropy">
    <figcaption class="figure-caption">Actor entropy</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p3.png" class="figure-img img-fluid" alt="Tokenization mismatch mean">
    <figcaption class="figure-caption">Tokenization mismatch mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p4.png" class="figure-img img-fluid" alt="Rollout probs diff mean">
    <figcaption class="figure-caption">Rollout probs diff mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p5.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_filter/p6.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
<p>We observe that gradient norms remain below 1.0 throughout training, aside from a few isolated spikes. Training mean reward increases to around 0.8 and validation task completion rate reaches approximately 60% after 500 RL iterations. A behavior change occurs after roughly 300 iterations, where entropy, tokenization mismatch, and rollout probability differences begin to increase. At the moment, I do not have a clear explanation for this behavior.</p>
<p>One remaining question is why a non-zero tokenization mismatch persists even when token IDs are obtained directly from vLLM. This appears to arise from the multi-step interaction loop: response token IDs are detokenized to interact with the environment and then re-tokenized when generating the next step. The mismatch occurs during this detokenization–re-tokenization cycle. Addressing this fully likely requires keeping the entire multi-step loop in token space within the <code>rllm</code> library.</p>
<p>Finally, I explore what happens when truncated trajectories are kept rather than discarded by setting <code>filter_token_mismatch=False</code>:</p>
<div class="figure-grid">
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p1.png" class="figure-img img-fluid" alt="Actor gradient norm">
    <figcaption class="figure-caption">Actor gradient norm</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p2.png" class="figure-img img-fluid" alt="Actor entropy">
    <figcaption class="figure-caption">Actor entropy</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p3.png" class="figure-img img-fluid" alt="Tokenization mismatch mean">
    <figcaption class="figure-caption">Tokenization mismatch mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p4.png" class="figure-img img-fluid" alt="Rollout probs diff mean">
    <figcaption class="figure-caption">Rollout probs diff mean</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p5.png" class="figure-img img-fluid" alt="Training mean reward">
    <figcaption class="figure-caption">Training mean reward</figcaption>
  </figure>
  <figure class="figure text-center w-100">
    <img src="figures/drift_no_filter/p6.png" class="figure-img img-fluid" alt="Validation task completion score">
    <figcaption class="figure-caption">Validation task completion score</figcaption>
  </figure>
</div>
<p>Interestingly, we observe notably different behavior in entropy, tokenization mismatch, and rollout probability differences compared to the previous setting. However, overall training stability remains strong, and the validation task completion rate improves slightly to approximately 61–64% after 500 RL iterations. This suggests that even truncated trajectories may still contain useful learning signals, although further analysis is needed to fully understand their impact.</p>
</section>
<section id="rollout-correction-methods-for-traininginference-mismatch" class="level3">
<h3 class="anchored" data-anchor-id="rollout-correction-methods-for-traininginference-mismatch">++ Rollout correction methods for training–inference mismatch</h3>
<p>So far, none of the experiments presented apply rollout correction methods to explicitly address training–inference mismatch. Inspecting the rollout probability difference plots, we observe a non-negligible mismatch between inference-time and training-time distributions, even though overall training stability has improved substantially.</p>
<p>A careful study of training–inference mismatch and rollout correction methods is left for future work, given the growing body of recent methods in this area: <a href="https://verl.readthedocs.io/en/latest/algo/rollout_corr_math.html" class="uri">https://verl.readthedocs.io/en/latest/algo/rollout_corr_math.html</a>.</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>